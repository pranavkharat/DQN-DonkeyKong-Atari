# ðŸ§  Methodology

## Table of Contents
- [Overview](#overview)
- [Environment Setup](#environment-setup)
- [DQN Architecture](#dqn-architecture)
- [Training Framework](#training-framework)
- [Optimization Techniques](#optimization-techniques)
- [Experimental Design](#experimental-design)
- [Implementation Details](#implementation-details)
- [Conclusion](#conclusion)

---

## Overview

This project implements a **Deep Q-Network (DQN)** agent to play **Atari DonkeyKong-v5**, inspired by the seminal work of *Mnih et al. (2015)*.  
The goal is to analyze how **hyperparameters** and **exploration strategies** affect learning dynamics and performance.

### Key Research Questions
1. How do Bellman parameters (Î±, Î³) influence convergence speed?  
2. What is the optimal exploration strategy for DonkeyKong?  
3. How does **Boltzmann exploration** compare to **Îµ-greedy**?  
4. How does the **discount factor (Î³)** impact performance in short-horizon arcade games?

---

## Environment Setup

### ðŸŽ® Atari DonkeyKong-v5

| Property | Description |
|-----------|--------------|
| **State Space** | 210Ã—160Ã—3 RGB frames |
| **Action Space** | 18 discrete actions (movement + jump + fire) |
| **Rewards** | Positive for progress, zero/negative for failure |
| **Episode Ends** | On death or timeout |

### ðŸ§© Preprocessing Pipeline

**1ï¸âƒ£ Frame Preprocessing**
```python
def preprocess_frame(frame):
    gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])
    resized = resize(gray, (84, 84))
    return resized / 255.0
```
- Converts RGB â†’ grayscale  
- Reduces input from 100,800 â†’ 7,056 pixels  
- Normalization stabilizes training  

**2ï¸âƒ£ Frame Stacking**
```python
def stack_frames(frames, stack_size=4):
    return np.stack(frames[-stack_size:], axis=0)
```
- Adds temporal context (velocity/motion)  
- Output shape: (4, 84, 84)

**3ï¸âƒ£ Frame Skip (4x Speedup)**
```python
def frame_skip(env, action, skip=4):
    total_reward, frames = 0, []
    for _ in range(skip):
        obs, reward, done, info = env.step(action)
        total_reward += reward
        frames.append(obs)
        if done: break
    return np.max(frames[-2:], axis=0), total_reward, done, info
```
- 4Ã— faster training  
- Reduces redundancy  
- Mitigates Atari flickering artifacts  

---

## DQN Architecture

**Input:** (4, 84, 84)  
**Layers:**
1. Conv1 â€” 32 filters, 8Ã—8, stride 4  
2. Conv2 â€” 64 filters, 4Ã—4, stride 2  
3. Conv3 â€” 64 filters, 3Ã—3, stride 1  
4. FC1 â€” 512 units, ReLU  
5. FC2 â€” 18 outputs (Q-values per action)

**Parameters:** ~1.7M  
**Initialization:** He for conv layers, Xavier for FC layers, bias = 0.01  

---

## Training Framework

### ðŸ” Bellman Equation
```python
Q(s,a) â† Q(s,a) + Î± [r + Î³ max_a' Q_target(s',a') - Q(s,a)]
```

Implemented via:
```python
loss = MSE(Q(s,a), r + Î³ max_a' Q_target(s',a'))
```

- **Q-Network:** Learns policy values  
- **Target Network:** Updated every 1,000 steps  
- **Optimizer:** Adam (lr = 0.00025)  
- **Gradient Clipping:** max_norm = 10  

### ðŸ§  Experience Replay
Stores `(s, a, r, s', done)` transitions.  
- Capacity: 30,000  
- Minimum warmup: 6,000  
- Random minibatch sampling for decorrelation  

### ðŸŽ¯ Exploration Strategies
**Epsilon-Greedy:**
- Îµ_start = 1.0 â†’ Îµ_min = 0.1  
- Decay = 0.995  

**Boltzmann (Softmax):**
- Ï„_start = 1.0 â†’ Ï„_min = 0.1  
- Ï„_decay = 0.995  
- Weighted action probabilities improve sample efficiency  

---

## Optimization Techniques

| Technique | Benefit |
|------------|----------|
| **Frame Skip (Ã—4)** | 3Ã— faster training |
| **Replay Buffer (30k)** | 40% faster sampling |
| **Gradient Clipping** | Prevents instability |
| **GPU Acceleration** | Tesla P100 / A100 |
| **Reduced Warmup** | Earlier policy learning |

---

## Experimental Design

### Baseline Configuration
| Parameter | Value |
|------------|--------|
| Episodes | 1,500 |
| Steps/Episode | 1,000 |
| Î± (LR) | 0.00025 |
| Î³ | 0.99 |
| Îµ_start | 1.0 |
| Îµ_min | 0.1 |
| Îµ_decay | 0.995 |

**Baseline Reward:** ~122 (avg last 100 episodes)

### Experiment Groups

#### 1ï¸âƒ£ Bellman Equation
- LR â†‘ â†’ 0.0005 â†’ Faster convergence  
- Î³ â†“ â†’ 0.95 â†’ Focus on immediate rewards  

#### 2ï¸âƒ£ Exploration
- Îµ_min â†“ â†’ 0.01 â†’ More exploration  
- Îµ_decay â†“ â†’ 0.99 â†’ Faster greediness  

#### 3ï¸âƒ£ Policy Exploration
- Replace Îµ-greedy with **Boltzmann**  
- Result: **+2800% performance improvement**

### Evaluation Metrics
- Average Reward (100-ep moving avg)  
- Convergence Speed  
- Max Reward  
- Reward Variance  
- Training Time  

---

## Implementation Details

**Stack:**
- PyTorch â‰¥ 2.0  
- Gymnasium â‰¥ 0.29  
- ALE-py â‰¥ 0.10  
- NumPy â‰¥ 1.24  
- Matplotlib â‰¥ 3.7  

**Platform:** Kaggle GPU (Tesla P100)  
**Python:** 3.10+  
**Seed:** 42 (for reproducibility)  

**Checkpointing:**
- Every 500 episodes  
- Includes weights, optimizer, replay buffer metadata  

**Memory Management:**
- Auto cleanup of older checkpoints  
- Limited buffer for recent 1,000 episodes  

---

## Conclusion

This methodology integrates a **DQN training pipeline** optimized for speed and stability.  
Through systematic hyperparameter tuning and exploration analysis, the agent achieves substantial improvements in performance and convergence efficiency for the **DonkeyKong-v5** environment.

### ðŸ”‘ Key Contributions
1. 3Ã— faster DQN training with minimal accuracy loss  
2. Comprehensive study of Î±, Î³, Îµ, Ï„ parameters  
3. Empirical validation: Boltzmann > Epsilon-Greedy  
4. Fully reproducible experimental framework  

---

**Author:** Pranav  
**Institution:** Northeastern University  
**Date:** November 2025  
